{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cara 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Read 204570 from https://www.detik.com\n",
      "Downloaded 80 in 1.5578665733337402 seconds\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import requests\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "thread_local = threading.local()\n",
    "\n",
    "\n",
    "def get_session():\n",
    "    if not hasattr(thread_local, \"session\"):\n",
    "        thread_local.session = requests.Session()\n",
    "    return thread_local.session\n",
    "\n",
    "\n",
    "def download_site(url):\n",
    "    session = get_session()\n",
    "    with session.get(url) as response:\n",
    "        print(f\"Read {len(response.content)} from {url}\")\n",
    "\n",
    "\n",
    "def download_all_sites(sites):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        executor.map(download_site, sites)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sites = [\n",
    "        \"https://www.detik.com\",\n",
    "    ] * 80\n",
    "    start_time = time.time()\n",
    "    download_all_sites(sites)\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Downloaded {len(sites)} in {duration} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cara 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def simple_get(url='https://www.detik.com'):\n",
    "    \"\"\"\n",
    "    Attempts to get the content at `url` by making an HTTP GET request.\n",
    "    If the content-type of response is some kind of HTML/XML, return the\n",
    "    text content, otherwise return None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with closing(get(url, stream=True)) as resp:\n",
    "            if is_good_response(resp):\n",
    "                return resp.content\n",
    "            else:\n",
    "                return None\n",
    "    except RequestException as e:\n",
    "        log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n",
    "        return None\n",
    "def is_good_response(resp):\n",
    "    \"\"\"\n",
    "    Returns true if the response seems to be HTML, false otherwise\n",
    "    \"\"\"\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200 \n",
    "            and content_type is not None \n",
    "            and content_type.find('html') > -1)\n",
    "def log_error(e):\n",
    "    \"\"\"\n",
    "    It is always a good idea to log errors. \n",
    "    This function just prints them, but you can\n",
    "    make it do anything.\n",
    "    \"\"\"\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_html = simple_get('https://www.detik.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202989"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cara 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scraper\n",
      "  Downloading scraper-0.1.0.tar.gz (2.6 kB)\n",
      "Requirement already satisfied: lxml>=3.0.1 in c:\\users\\evangs mailoa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scraper) (4.7.1)\n",
      "Using legacy 'setup.py install' for scraper, since package 'wheel' is not installed.\n",
      "Installing collected packages: scraper\n",
      "    Running setup.py install for scraper: started\n",
      "    Running setup.py install for scraper: finished with status 'done'\n",
      "Successfully installed scraper-0.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\EVANGS MAILOA\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'connect_to_base'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\EVANGS~1\\AppData\\Local\\Temp/ipykernel_10684/1208157346.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfutures\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msleep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mconnect_to_base\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_driver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_html\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrite_to_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'connect_to_base'"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import sys\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from time import sleep, time\n",
    "\n",
    "from scrapers.scraper import connect_to_base, get_driver, parse_html, write_to_file\n",
    "\n",
    "\n",
    "def run_process(filename, headless):\n",
    "\n",
    "    # init browser\n",
    "    browser = get_driver(headless)\n",
    "\n",
    "    if connect_to_base(browser):\n",
    "        sleep(2)\n",
    "        html = browser.page_source\n",
    "        output_list = parse_html(html)\n",
    "        write_to_file(output_list, filename)\n",
    "\n",
    "        # exit\n",
    "        browser.quit()\n",
    "    else:\n",
    "        print(\"Error connecting to Wikipedia\")\n",
    "        browser.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # headless mode?\n",
    "    headless = False\n",
    "    if len(sys.argv) > 1:\n",
    "        if sys.argv[1] == \"headless\":\n",
    "            print(\"Running in headless mode\")\n",
    "            headless = True\n",
    "\n",
    "    # set variables\n",
    "    start_time = time()\n",
    "    output_timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    output_filename = f\"output_{output_timestamp}.csv\"\n",
    "    futures = []\n",
    "\n",
    "    # scrape and crawl\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for number in range(1, 21):\n",
    "            futures.append(\n",
    "                executor.submit(run_process, output_filename, headless)\n",
    "            )\n",
    "\n",
    "    wait(futures)\n",
    "    end_time = time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed run time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cara 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken: 0.168551s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "URLs = [ 'https:://www.detik.com' ] # A long list of URLs.\n",
    "def parse(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, 'lxml')\n",
    "    return soup.find_all('a')\n",
    "with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    start = time.time()\n",
    "    futures = [ executor.submit(parse, url) for url in URLs ]\n",
    "    results = []\n",
    "    for result in as_completed(futures):\n",
    "        results.append(result)\n",
    "    end = time.time()\n",
    "    print(\"Time Taken: {:.6f}s\".format(end-start))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90f1bec0ab3333bf3bb7975c69a4c8770889fb3e307d7059d5773e89f2027833"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
