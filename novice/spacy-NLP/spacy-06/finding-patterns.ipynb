{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1409e73-d42b-49c4-a66b-c7ba3ac4c16d",
   "metadata": {},
   "source": [
    "# Pola Kalimat\n",
    "\n",
    "Dalam melakukan analisis untuk memahami kalimat, kita bisa menggunakan *word sequence patterns*. Fungsi utamanya biasanya digunakan untuk klasifikasi dan menghasilkan teks. Selain itu ada teknik *walking the syntactic dependency tree* untuk mengambil informasi tertentu dalam suatu kalimat.\n",
    "\n",
    "**Catatan**: materi pada notebook ini diambil dari https://nostarch.com/NLPPython - bab 6 dan disesuaikan dengan versi spaCy terbaru (3.2.x).\n",
    "\n",
    "## Word Sequence Patterns\n",
    "\n",
    "Pola ini digunakan untuk mendeteksi dan membuat klasifikasi suatu kalimat. Sebagai contoh, kita bisa mengklasifikasikan suatu kalimat sebagai *kalimat tanya yang menanyakan kemampuan*, dan lain-lain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76d8ba3d-a949-4d57-9785-7d3d7ebeef2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We You nsubj nominal subject\n",
      "can must aux auxiliary\n",
      "overtake specify ROOT None\n",
      "them it dobj direct object\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc1 = nlp(u'We can overtake them.')\n",
    "doc2 = nlp(u'You must specify it.')\n",
    "\n",
    "for i in range(len(doc1)-1):\n",
    "    if doc1[i].dep_ == doc2[i].dep_:\n",
    "        print(doc1[i].text, doc2[i].text, doc1[i].dep_, spacy.explain(doc1[i].dep_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10ba986a-6e94-4123-ba1e-a7eedb5e39c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We You PRON pronoun\n",
      "can must AUX auxiliary\n",
      "overtake specify VERB verb\n",
      "them it PRON pronoun\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc1 = nlp(u'We can overtake them.')\n",
    "doc2 = nlp(u'You must specify it.')\n",
    "\n",
    "for i in range(len(doc1)-1):\n",
    "    if doc1[i].pos_ == doc2[i].pos_:\n",
    "        print(doc1[i].text, doc2[i].text, doc1[i].pos_, spacy.explain(doc1[i].pos_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61fb03d-a8cc-4fe3-901e-2e8fbdcf38db",
   "metadata": {},
   "source": [
    "Kedua script di atas menunjukkan bahwa meski kedua kalimat tersebut mempunyai makna yang berbeda, tetapi mempunyai struktur pola kalimat yang sama. Script di bawah ini akan melakukan hal yang sama tetapi sudah dalam bentuk beberapa kalimat yang digabung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7112abe3-9280-4033-b523-a6f3e3266606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We You nsubj nominal subject\n",
      "can must aux auxiliary\n",
      "overtake specify ROOT None\n",
      "them it dobj direct object\n",
      "We I nsubj nominal subject\n",
      "can could aux auxiliary\n",
      "overtake do ROOT None\n",
      "them it dobj direct object\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(u'We can overtake them. You must specify it. I could do it.')\n",
    "\n",
    "sents = list(doc.sents)\n",
    "for sent in sents[1:]:\n",
    "    for i in range(len(sents[0])-1):\n",
    "        if sents[0][i].dep_ == sent[i].dep_:\n",
    "            print(sents[0][i].text, sent[i].text, sents[0][i].dep_, spacy.explain(sents[0][i].dep_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177acf21-1d27-42fd-a394-9add3d5e9bb2",
   "metadata": {},
   "source": [
    "Kita bisa melakukan pemeriksaan apakah suatu kalimat mempunyai pola tertentu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f11c953-6d44-4aec-a48c-ef6fd44b0938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def dep_pattern(doc):\n",
    "    for i in range(len(doc)-1):\n",
    "        if doc[i].dep_ == 'nsubj' and doc[i+1].dep_ == 'aux' and  doc[i+2].dep_ == 'ROOT':\n",
    "            for tok in doc[i+2].children:\n",
    "                if tok.dep_ == 'dobj':\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "doc = nlp(u'We can overtake them.')\n",
    "\n",
    "if dep_pattern(doc):\n",
    "  print('Found')\n",
    "else:\n",
    "  print('Not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2043d5-bf90-46cb-9e61-d8964b91af3c",
   "metadata": {},
   "source": [
    "Memeriksa pola seperti di atas bisa dilakukan dengan menggunakan *Matcher*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e75cda49-30ce-4234-851e-fdb2617cadfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span:  We can overtake\n",
      "The positions in the doc are:  0 - 3\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [[{\"DEP\": \"nsubj\"}, {\"DEP\": \"aux\"}, {\"DEP\": \"ROOT\"}]]\n",
    "\n",
    "matcher.add(\"NsubjAuxRoot\", pattern)\n",
    "\n",
    "doc = nlp(u\"We can overtake them.\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(\"Span: \", span.text)\n",
    "    print(\"The positions in the doc are: \", start, \"-\", end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb63d55-71a8-429d-9013-d9b36612e4f4",
   "metadata": {},
   "source": [
    "Pola bisa kita definisikan lebih dari satu dan kemudian digabungkan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d736bfa9-c7d8-41d8-bb66-e4fea731868b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found\n",
      "Not found\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def dep_pattern(doc):\n",
    "    for i in range(len(doc)-1):\n",
    "        if doc[i].dep_ == 'nsubj' and doc[i+1].dep_ == 'aux' and  doc[i+2].dep_ == 'ROOT':\n",
    "            for tok in doc[i+2].children:\n",
    "                if tok.dep_ == 'dobj':\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def pos_pattern(doc):\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'nsubj' and token.tag_ != 'PRP':\n",
    "            return False\n",
    "        if token.dep_ == 'aux' and token.tag_ != 'MD':\n",
    "            return False\n",
    "        if token.dep_ == 'ROOT' and token.tag_ != 'VB':\n",
    "            return False\n",
    "        if token.dep_ == 'dobj' and token.tag_ != 'PRP':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "#Testing code\n",
    "\n",
    "doc = nlp(u'We can overtake them.')\n",
    "if dep_pattern(doc) and pos_pattern(doc):\n",
    "    print('Found')\n",
    "else:\n",
    "    print('Not found')\n",
    "\n",
    "doc = nlp(u'I might send them a card as a reminder.')\n",
    "if dep_pattern(doc) and pos_pattern(doc):\n",
    "    print('Found')\n",
    "else:\n",
    "    print('Not found')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636ca2a7-ffc3-4ece-800d-f03f107018cb",
   "metadata": {},
   "source": [
    "Script berikut ini akan menentukan pola untuk *pronoun*. Mengapa perlu dilakukan? karena seringkali dalam suatu kalimat, setelah dimunculkan suatu kata, berikutnya akan direferensi menggunakan *pronoun*. Contoh: \n",
    "\n",
    "```\n",
    "The trucks are traveling slowly. We can overtake them\n",
    "```\n",
    "\n",
    "Pada kalimat tersebut, *them* merujuk pada *trucks* yang sudah disebutkan sebelumnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91faec76-510c-40a3-93bb-1ef432c42a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: the pronoun in position of direct object is plural\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def dep_pattern(doc):\n",
    "    for i in range(len(doc)-1):\n",
    "        if doc[i].dep_ == 'nsubj' and doc[i+1].dep_ == 'aux' and  doc[i+2].dep_ == 'ROOT':\n",
    "            for tok in doc[i+2].children:\n",
    "                if tok.dep_ == 'dobj':\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def pos_pattern(doc):\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'nsubj' and token.tag_ != 'PRP':\n",
    "            return False\n",
    "        if token.dep_ == 'aux' and token.tag_ != 'MD':\n",
    "            return False\n",
    "        if token.dep_ == 'ROOT' and token.tag_ != 'VB':\n",
    "            return False\n",
    "        if token.dep_ == 'dobj' and token.tag_ != 'PRP':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def pron_pattern(doc):\n",
    "    plural = ['we','us','they','them']\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'dobj' and token.tag_ == 'PRP':\n",
    "            if token.text in plural:\n",
    "                return 'plural'\n",
    "            else:\n",
    "                return 'singular'\n",
    "    return 'not found'\n",
    "\n",
    "doc = nlp(u'We can overtake them.')\n",
    "\n",
    "if dep_pattern(doc) and pos_pattern(doc):\n",
    "    print('Found:', 'the pronoun in position of direct object is',\n",
    "    pron_pattern(doc))\n",
    "else:\n",
    "    print('Not found')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feb1f9d-ab0c-4938-ab53-b83725be6498",
   "metadata": {},
   "source": [
    "## Menggunakan *Word Sequence Patterns* untuk Menghasilkan Teks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb471d25-c2a2-4092-8cc5-e0fe7ad48989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can recognize symbols too.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def dep_pattern(doc):\n",
    "    for i in range(len(doc)-1):\n",
    "        if doc[i].dep_ == 'nsubj' and doc[i+1].dep_ == 'aux' and  doc[i+2].dep_ == 'ROOT':\n",
    "            for tok in doc[i+2].children:\n",
    "                if tok.dep_ == 'dobj':\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def pos_pattern(doc):\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'nsubj' and token.tag_ != 'PRP':\n",
    "            return False\n",
    "        if token.dep_ == 'aux' and token.tag_ != 'MD':\n",
    "            return False\n",
    "        if token.dep_ == 'ROOT' and token.tag_ != 'VB':\n",
    "            return False\n",
    "        if token.dep_ == 'dobj' and token.tag_ != 'PRP':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def pron_pattern(doc):\n",
    "    plural = ['we','us','they','them']\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'dobj' and token.tag_ == 'PRP':\n",
    "            if token.text in plural:\n",
    "                return 'plural'\n",
    "            else:\n",
    "                return 'singular'\n",
    "    return 'not found'\n",
    "\n",
    "def find_noun(sents, num):\n",
    "    if num == 'plural':\n",
    "        taglist = ['NNS','NNPS']\n",
    "    if num == 'singular':\n",
    "        taglist = ['NN','NNP']\n",
    "    for sent in reversed(sents):\n",
    "        for token in sent:\n",
    "            if token.tag_ in taglist:\n",
    "                return token.text\n",
    "    return 'Noun not found'\n",
    "\n",
    "def gen_utterance(doc, noun):\n",
    "    sent = ''\n",
    "    for i,token in enumerate(doc):\n",
    "        if token.dep_ == 'dobj' and token.tag_ == 'PRP':\n",
    "            sent = doc[:i].text + ' ' + noun + ' ' + doc[i+1:len(doc)-2].text + 'too.'\n",
    "            return sent\n",
    "    return 'Failed to generate an utterance' \n",
    "\n",
    "doc = nlp(u'The symbols are clearly distinguishable. I can recognize them promptly.')\n",
    "\n",
    "sents = list(doc.sents)\n",
    "\n",
    "response = ''\n",
    "\n",
    "noun = ''\n",
    "\n",
    "for i, sent in enumerate(sents):\n",
    "    if dep_pattern(sent) and pos_pattern(sent):\n",
    "        noun = find_noun(sents[:i], pron_pattern(sent))\n",
    "        if noun != 'Noun not found':\n",
    "            response = gen_utterance(sents[i],noun)\n",
    "            break\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb7954f-0610-427c-99dd-2019e9d021c9",
   "metadata": {},
   "source": [
    "## Information Extraction - *Walking Dependency Tree*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99053736-e2b1-4fa4-9e1d-a8b6aca8250c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems the user wants a ticket to Berlin\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#Here's the function that figures out the destination\n",
    "\n",
    "def det_destination(doc):\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.ent_type != 0 and token.ent_type_ == 'GPE':\n",
    "            while True:\n",
    "                token = token.head\n",
    "                if token.text == 'to':\n",
    "                    return doc[i].text\n",
    "                if token.head == token:\n",
    "                    return 'Failed to determine'\n",
    "    return 'Failed to determine'\n",
    "\n",
    "#Testing the det_destination function\n",
    "\n",
    "doc = nlp(u'I am going to the conference in Berlin.')\n",
    "\n",
    "dest = det_destination(doc)\n",
    "\n",
    "print('It seems the user wants a ticket to ' + dest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aab937-e345-45c9-92f2-de547832d34b",
   "metadata": {},
   "source": [
    "### Meringkas Teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f0a9870-5b76-4227-9dcc-3f51994e7d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The product sales hit 18.6 million units sold.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(u\"The product sales hit a new record in the first quarter, with 18.6 million units sold.\")\n",
    "\n",
    "phrase = ''\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ == 'NUM':\n",
    "        while True:\n",
    "            phrase = phrase + ' ' + token.text\n",
    "            token = token.head\n",
    "            if token not in list(token.head.lefts):\n",
    "                phrase = phrase + ' ' + token.text\n",
    "                if list(token.rights):\n",
    "                    phrase = phrase + ' ' + doc[token.i+1:].text\n",
    "                break\n",
    "        break\n",
    "\n",
    "while True:\n",
    "    token = doc[token.i].head\n",
    "    if token.pos_ != 'ADP':\n",
    "        phrase = token.text + phrase\n",
    "    if token.dep_ == 'ROOT':\n",
    "        break\n",
    "\n",
    "for tok in token.lefts:\n",
    "    if tok.dep_ == 'nsubj':\n",
    "        phrase = ' '.join([tok.text for tok in tok.lefts]) + ' ' + tok.text + ' '+ phrase\n",
    "        break\n",
    "\n",
    "print(phrase.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28e6246-ddb6-402b-9e6c-e9532d8d9c80",
   "metadata": {},
   "source": [
    "Jika sebelumnya, penentuan tiket ke Berlin berdasarkan *to* + *GPE*, maka bisa diperbaiki menggunakan konteks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76856a9f-2f0c-42a5-90b2-2101cf462b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When do you need to be in Berlin?\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def det_destination(doc):\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.ent_type != 0 and token.ent_type_ == 'GPE':\n",
    "            while True:\n",
    "                token = token.head\n",
    "                if token.text == 'to':\n",
    "                    return doc[i].text\n",
    "                if token.head == token:\n",
    "                    return 'Failed to determine'\n",
    "    return 'Failed to determine'\n",
    "\n",
    "def guess_destination(doc):\n",
    "    for token in doc:\n",
    "        if token.ent_type != 0 and token.ent_type_ == 'GPE':\n",
    "            return token.text\n",
    "    return 'Failed to determine'\n",
    "\n",
    "def gen_response(doc):\n",
    "    dest = det_destination(doc)\n",
    "    if dest != 'Failed to determine':\n",
    "        return 'When do you need to be in ' + dest + '?'\n",
    "    dest = guess_destination(doc)\n",
    "    if dest != 'Failed to determine':\n",
    "        return 'You want a ticket to ' + dest +', right?'\n",
    "    return 'Are you flying somewhere?'\n",
    "\n",
    "doc = nlp(u'I am going to the conference in Berlin.')\n",
    "\n",
    "print(gen_response(doc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95337804-8635-437a-aa15-27d5d16ae508",
   "metadata": {},
   "source": [
    "## Modifier untuk Makna yang Lebih Spesifik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd119b74-fa0a-4d60-a0a0-390d7696edfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list of adjectival modifiers for word fruit: ['nice', 'exotic']\n",
      "The list of GPE names applicable to word fruit as postmodifiers: ['Africa']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(u\"Kiwano has jelly-like flesh with a refreshingly fruity taste. This is a nice exotic fruit from Africa. It is definitely worth trying.\")\n",
    "\n",
    "fruit_adjectives = []\n",
    "fruit_origins = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.text == 'fruit':\n",
    "        fruit_adjectives = fruit_adjectives + [modifier.text for modifier in token.lefts if modifier.pos_ == 'ADJ']\n",
    "        fruit_origins = fruit_origins + [doc[modifier.i + 1].text for modifier in token.rights if modifier.text == 'from' and doc [modifier.i + 1].ent_type != 0]\n",
    "\n",
    "print('The list of adjectival modifiers for word fruit:', fruit_adjectives)\n",
    "print('The list of GPE names applicable to word fruit as postmodifiers:', fruit_origins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7842a3-f5c3-47c5-95dc-e1838b5714f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
