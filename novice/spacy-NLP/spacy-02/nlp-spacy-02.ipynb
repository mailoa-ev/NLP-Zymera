{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04246f9e-35cc-4e9a-a1c7-e2ba765241b4",
   "metadata": {},
   "source": [
    "# Text Processing Pipelines\n",
    "\n",
    "![Text processing pipelines](text-processing-pipelines.jpg)\n",
    "\n",
    "Text => nlp => Doc. Proses ini akan menghasilkan token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23f34b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "world\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca74a89d",
   "metadata": {},
   "source": [
    "Setelah itu, baru dilakukan berbagai pemrosesan. Informasi tentang processing pipelines: https://spacy.io/usage/processing-pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bc06cc",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization adalah proses untuk mencari bentuk kata dasar. Proses ini dilaksanakan oleh lemmatizer. Pada spaCy, lemmatizer dilakukan oleh komponen lmmatizer. Informasi lengkap ada di https://spacy.io/api/lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6a6c0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this this\n",
      "product product\n",
      "integrates integrate\n",
      "both both\n",
      "libraries library\n",
      "for for\n",
      "downloading download\n",
      "and and\n",
      "applying apply\n",
      "patches patch\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(u'this product integrates both libraries for downloading and applying patches')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd519fd",
   "metadata": {},
   "source": [
    "Perhatikan, lemmatization ini bisa menjadi titik awal dari intent recognition. Misal kita akan memproses masukan dari user yang akan membeli tiket. Tiket bisa berupa tiket bis, pesawat, maupun kendaraan lain. Dengan demikian, diperlukan data tentang:\n",
    "\n",
    "1. Kendaraan yang diinginkan: fly => terbang, naik pesawat\n",
    "2. Kemana? => to menunjukkan tujuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5882608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['token:I lemma:I', 'token:am lemma:be', 'token:flying lemma:fly', 'token:to lemma:to', 'token:Frisco lemma:San Francisco']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# orth is simply an integer that indicates \n",
    "# the index of the occurrence of the word that \n",
    "# is kept in the spacy. tokens\n",
    "from spacy.symbols import LOWER, LEMMA\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp.get_pipe(\"attribute_ruler\").add([[{\"LOWER\": \"frisco\"}]], {\"LEMMA\": \"San Francisco\"})\n",
    "\n",
    "doc = nlp(u'I am flying to Frisco')\n",
    "\n",
    "print(['token:%s lemma:%s' % (t.text, t.lemma_) for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a97178-2634-423c-8519-b3b91dcae55c",
   "metadata": {},
   "source": [
    "## Tagger\n",
    "\n",
    "Merupakan komponen untuk *PoS Tagging*. Dokumentasi lengkap: https://spacy.io/api/tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c74db295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:I lemma:I pos:PRON tag:PRP\n",
      "token:have lemma:have pos:AUX tag:VBP\n",
      "token:flown lemma:fly pos:VERB tag:VBN\n",
      "token:to lemma:to pos:ADP tag:IN\n",
      "token:LA lemma:LA pos:PROPN tag:NNP\n",
      "token:. lemma:. pos:PUNCT tag:.\n",
      "token:Now lemma:now pos:ADV tag:RB\n",
      "token:I lemma:I pos:PRON tag:PRP\n",
      "token:am lemma:be pos:AUX tag:VBP\n",
      "token:flying lemma:fly pos:VERB tag:VBG\n",
      "token:to lemma:to pos:ADP tag:IN\n",
      "token:Frisco lemma:San Francisco pos:PROPN tag:NNP\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# orth is simply an integer that indicates \n",
    "# the index of the occurrence of the word that \n",
    "# is kept in the spacy. tokens\n",
    "from spacy.symbols import LOWER, LEMMA\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp.get_pipe(\"attribute_ruler\").add([[{\"LOWER\": \"frisco\"}]], {\"LEMMA\": \"San Francisco\"})\n",
    "\n",
    "doc = nlp(u'I have flown to LA. Now I am flying to Frisco')\n",
    "\n",
    "for t in doc:\n",
    "    print('token:%s lemma:%s pos:%s tag:%s' % (t.text, t.lemma_, t.pos_, t.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbd2b9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'verb, gerund or present participle'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "#spacy.explain(\"PRON\") \n",
    "#spacy.explain(\"PRP\") \n",
    "#spacy.explain(\"AUX\")\n",
    "#spacy.explain(\"VBP\") \n",
    "#spacy.explain(\"VBN\")\n",
    "#spacy.explain(\"ADP\") \n",
    "#spacy.explain(\"IN\") \n",
    "#spacy.explain(\"PROPN\") \n",
    "#spacy.explain(\"NNP\") \n",
    "#spacy.explain(\"PUNCT\") \n",
    "#spacy.explain(\"ADV\") \n",
    "#spacy.explain(\"RB\") \n",
    "#spacy.explain(\"VERB\") \n",
    "spacy.explain(\"VBG\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef156289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:I lemma:I pos:PRON dep:nsubj\n",
      "token:have lemma:have pos:AUX dep:aux\n",
      "token:flown lemma:fly pos:VERB dep:ROOT\n",
      "token:to lemma:to pos:ADP dep:prep\n",
      "token:LA lemma:LA pos:PROPN dep:pobj\n",
      "token:. lemma:. pos:PUNCT dep:punct\n",
      "token:Now lemma:now pos:ADV dep:advmod\n",
      "token:I lemma:I pos:PRON dep:nsubj\n",
      "token:am lemma:be pos:AUX dep:aux\n",
      "token:flying lemma:fly pos:VERB dep:ROOT\n",
      "token:to lemma:to pos:ADP dep:prep\n",
      "token:Frisco lemma:San Francisco pos:PROPN dep:pobj\n"
     ]
    }
   ],
   "source": [
    "for t in doc:\n",
    "    print('token:%s lemma:%s pos:%s dep:%s' % (t.text, t.lemma_, t.pos_, t.dep_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df2577b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token head text:flown dependency:nsubj text:I\n",
      "token head text:flown dependency:aux text:have\n",
      "token head text:flown dependency:ROOT text:flown\n",
      "token head text:flown dependency:prep text:to\n",
      "token head text:to dependency:pobj text:LA\n",
      "token head text:flown dependency:punct text:.\n",
      "token head text:flying dependency:advmod text:Now\n",
      "token head text:flying dependency:nsubj text:I\n",
      "token head text:flying dependency:aux text:am\n",
      "token head text:flying dependency:ROOT text:flying\n",
      "token head text:flying dependency:prep text:to\n",
      "token head text:to dependency:pobj text:Frisco\n"
     ]
    }
   ],
   "source": [
    "for t in doc:\n",
    "    print('token head text:%s dependency:%s text:%s' % (t.head.text, t.dep_, t.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9bc28e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flown', 'LA']\n",
      "['flying', 'Frisco']\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print([w.text for w in sent if w.dep_ == 'ROOT' or w.dep_ == 'pobj'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4e9f15",
   "metadata": {},
   "source": [
    "## NER (Named Entity Recognition)\n",
    "\n",
    "Informasi lengkap: https://spacy.io/api/entityrecognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df534423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA GPE\n",
      "San Francisco ORG\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# orth is simply an integer that indicates \n",
    "# the index of the occurrence of the word that \n",
    "# is kept in the spacy. tokens\n",
    "from spacy.symbols import LOWER, LEMMA\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp.get_pipe(\"attribute_ruler\").add([[{\"LOWER\": \"frisco\"}]], {\"LEMMA\": \"San Francisco\"})\n",
    "\n",
    "doc = nlp(u'I have flown to LA. Now I am flying to Frisco')\n",
    "\n",
    "for token in doc:\n",
    "    if token.ent_type != 0:\n",
    "        print(token.lemma_, token.ent_type_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
